# type of the destination: `redshift` or `bigquery`
client_type="bigquery"
# dataset/schema name in the destination database where tables will be created
default_dataset="axies_1"
# where to keep pipeline state and working files
working_dir=".pipeline"
# where to take initial axies schema from
import_schema_path="."
# where to export current schema when any new tables/columns are added
export_schema_path="schema_export"

[ethereum]
# number of past blocks to get when pipeline is run for a first time
max_initial_blocks=10
# maximum number of blocks from the current block to get
max_blocks=300
